import { NextRequest, NextResponse } from "next/server";
import { createClient } from "@/lib/supabase/server";
import { scrapeWebsite, generateKnowledgeBase } from "@/lib/scraper/website-scraper";
import { isUrlAllowed } from "@/lib/security/validation";
import { withRateLimit } from "@/lib/security/rate-limiter";
import { resyncOrgAssistants } from "@/lib/knowledge-base";

/**
 * POST /api/v1/knowledge-base/scrape
 *
 * Scrapes a website and generates knowledge base content
 *
 * Body:
 * - url: string (required) - The website URL to scrape
 * - title: string (optional) - Title for the KB entry (defaults to domain name)
 * - maxPages: number (optional) - Maximum pages to scrape (default: 20)
 */
export async function POST(request: NextRequest) {
  try {
    // Rate limit - expensive operation
    const { allowed, headers } = withRateLimit(request, "/api/v1/knowledge-base/scrape", "expensive");
    if (!allowed) {
      return NextResponse.json(
        { error: "Too many requests. Please try again later." },
        { status: 429, headers }
      );
    }

    const supabase = await createClient();

    // Check authentication
    const {
      data: { user },
      error: authError,
    } = await supabase.auth.getUser();

    if (authError || !user) {
      return NextResponse.json(
        { error: "Unauthorized" },
        { status: 401 }
      );
    }

    // Get user's organization
    const { data: membership, error: membershipError } = await (supabase as any)
      .from("org_members")
      .select("organization_id")
      .eq("user_id", user.id)
      .single();

    if (membershipError || !membership) {
      return NextResponse.json(
        { error: "No organization found" },
        { status: 403 }
      );
    }

    const organizationId = membership.organization_id as string;

    // Parse request body
    const body = await request.json();
    const { url, title, maxPages = 20 } = body;

    if (!url) {
      return NextResponse.json(
        { error: "URL is required" },
        { status: 400 }
      );
    }

    // Validate URL format
    try {
      new URL(url);
    } catch {
      return NextResponse.json(
        { error: "Invalid URL provided" },
        { status: 400 }
      );
    }

    // SSRF Protection: Prevent scraping internal/private networks
    if (!isUrlAllowed(url)) {
      return NextResponse.json(
        { error: "URL not allowed - internal or private addresses are blocked" },
        { status: 400 }
      );
    }

    // Scrape the website
    const scrapedData = await scrapeWebsite(url, {
      maxPages: Math.min(maxPages, 50), // Cap at 50 pages
      maxDepth: 2,
    });

    // Generate knowledge base content
    const knowledgeBaseContent = generateKnowledgeBase(scrapedData);

    // Derive title from domain if not provided
    let entryTitle = title;
    if (!entryTitle) {
      try {
        entryTitle = new URL(url).hostname.replace(/^www\./, "");
      } catch {
        entryTitle = "Website Import";
      }
    }

    // Save as org-level KB entry (no assistant_id)
    const { error: insertError } = await (supabase as any)
      .from("knowledge_bases")
      .insert({
        organization_id: organizationId,
        assistant_id: null,
        title: entryTitle,
        source_type: "website",
        source_url: url,
        content: knowledgeBaseContent,
        metadata: {
          totalPages: scrapedData.totalPages,
          scrapedAt: scrapedData.scrapedAt,
          businessInfo: scrapedData.businessInfo,
        },
        is_active: true,
      });

    if (insertError) {
      console.error("Failed to save knowledge base:", insertError);
    }

    // Resync all org assistants with updated KB
    await resyncOrgAssistants(supabase, organizationId).catch((err) =>
      console.error("Failed to resync assistants:", err)
    );

    return NextResponse.json({
      success: true,
      data: {
        url: scrapedData.baseUrl,
        totalPages: scrapedData.totalPages,
        businessInfo: scrapedData.businessInfo,
        content: knowledgeBaseContent,
        contentLength: knowledgeBaseContent.length,
        pages: scrapedData.pages.map((p) => ({
          url: p.url,
          title: p.title,
          contentLength: p.content.length,
        })),
      },
    });
  } catch (error: any) {
    console.error("Scrape error:", error);
    // Don't expose internal error details to client
    return NextResponse.json(
      { error: "Failed to scrape website" },
      { status: 500 }
    );
  }
}
